for(i in 1:27){
# Predict the y-values of the current fold with the
# "best" model with i predictors
MSE.mod.1 = predict.reg(mod.1, newdata = valid, id=i) #Fold==j, when j=1 then it takes the subset 1, etc.
#Basically saying i want the best model with i predictors! and im going to predict on the best validation data set
# Calculate the Mean Squared Error (MSE) estimate
# for the jth fold and best i-variable model, store it
# in the cv_errors matrix we created above
v_errors[i] = mean((valid$hgt-MSE.mod.1)^2)
}
v_errors
dim(b)
#Create empty vector to store the values in later.
v_errors = rep(NA, 27)
#Make a model.
samp.mod = lm(hgt ~ . + sex*bii.di + sex*che.de + sex*for.gi, data=b)
summary(samp.mod)
#Use the training set to make a model.
mod.1 = regsubsets(hgt ~ . + sex:bii.di + sex:che.de + sex:for.gi, data=train, nvmax=27)
summary.mod.1 = summary(mod.1)
#Store summary of best models in outmat.reg.
outmat.reg = as.data.frame(summary.mod.1$outmat)
v_errors
for(i in 1:27){
# Predict the y-values of the current fold with the
# "best" model with i predictors
MSE.mod.1 = predict.reg(mod.1, newdata = valid, id=i) #Fold==j, when j=1 then it takes the subset 1, etc.
#Basically saying i want the best model with i predictors! and im going to predict on the best validation data set
# Calculate the Mean Squared Error (MSE) estimate
# for the jth fold and best i-variable model, store it
# in the cv_errors matrix we created above
v_errors[i] = mean((valid$hgt-MSE.mod.1)^2)
}
v_errors
b=bdims
set.seed(8929)
set.index = sample(c('train', 'validation', 'test'), prob = c(.5, .25, .25), replace = T, size = nrow(b))
b=bdims
set.seed(8929)
set.index = sample(c('train', 'validation', 'test'), prob = c(.5, .25, .25), replace = T, size = nrow(b))
train = b[set.index == 'train', ]
valid = b[set.index == 'validation', ]
test = b[set.index == 'test', ]
dim(train)
dim(valid)
dim(test)
dim(b)
#Create empty vector to store the values in later.
v_errors = rep(NA, 27)
#Make a model.
samp.mod = lm(hgt ~ . + sex*bii.di + sex*che.de + sex*for.gi, data=b)
summary(samp.mod)
#Use the training set to make a model.
mod.1 = regsubsets(hgt ~ . + sex:bii.di + sex:che.de + sex:for.gi, data=train, nvmax=27)
summary.mod.1 = summary(mod.1)
#Store summary of best models in outmat.reg.
outmat.reg = as.data.frame(summary.mod.1$outmat)
v_errors
for(i in 1:27){
# Predict the y-values of the current fold with the
# "best" model with i predictors
MSE.mod.1 = predict.reg(mod.1, newdata = valid, id=i) #Fold==j, when j=1 then it takes the subset 1, etc.
#Basically saying i want the best model with i predictors! and im going to predict on the best validation data set
# Calculate the Mean Squared Error (MSE) estimate
# for the jth fold and best i-variable model, store it
# in the cv_errors matrix we created above
v_errors[i] = mean((valid$hgt-MSE.mod.1)^2)
}
v_errors
outmat.reg
b=bdims
dim(b)
v_errors = rep(NA, 27)
v_errors
samp.mod = lm(hgt ~ . + sex*bii.di + sex*che.de + sex*for.gi, data=b)
summary(samp.mod)
v_errors
mod.1 = regsubsets(hgt ~ . + sex:bii.di + sex:che.de + sex:for.gi, data=train, nvmax=27)
summary.mod.1 = summary(mod.1)
outmat.reg = as.data.frame(summary.mod.1$outmat)
mean((valid$hgt-MSE.mod.1)^2)
v_errors[i] = mean((valid$hgt-MSE.mod.1)^2)
for(i in 1:27){
# Predict the y-values of the current fold with the
# "best" model with i predictors
MSE.mod.1 = predict.reg(mod.1, newdata = valid, id=i) #Fold==j, when j=1 then it takes the subset 1, etc.
#Basically saying i want the best model with i predictors! and im going to predict on the best validation data set
# Calculate the Mean Squared Error (MSE) estimate
# for the jth fold and best i-variable model, store it
# in the cv_errors matrix we created above
v_errors[i] = mean((valid$hgt-MSE.mod.1)^2)
}
v_errors
.
min.1 = which.min(v_errors)[1]
min.1
plot(v_errors, type='b', main="Graphed Mean Squared Error Values For Each Number of Predictors")
plot(v_errors, type='b', main="Graphed Mean Squared Error Values For Each Number of Predictors")
#Highlight the smallest one.
points(min.1, v_errors[min.1][1], col = "red", cex = 2, pch = 20)
i=12
predict.hgt = predict.reg(object = mod.1, newdata = valid, id=i)
mse.hgt=mean((valid$hgt-predict.hgt)^2)
mse.hgt
#Obtain the index number of which mean squared error (MSE) is the smallest.
min.1 = which.min(v_errors)[1]
min.1
#Plot the MSE
plot(v_errors, type='b', main="Graphed Mean Squared Error Values For Each Number of Predictors")
#Highlight the smallest one.
points(min.1, v_errors[min.1][1], col = "red", cex = 2, pch = 20)
#Indicate number of predictors as 12.
i=12
#Obtained predicted values with validation data set and linear model mod.1.
predict.hgt = predict.reg(object = mod.1, newdata = valid, id=i)
#Obtain MSE.
mse.hgt=mean((valid$hgt-predict.hgt)^2)
mse.hgt
mse.hgt=mean((valid$hgt-predict.hgt)^2)
mse.hgt
#Use the training set to make a model.
mod.1 = regsubsets(hgt ~ . + sex:bii.di + sex:che.de + sex:for.gi, data=train, nvmax=27)
summary.mod.1 = summary(mod.1)
#Store summary of best models in outmat.reg.
outmat.reg = as.data.frame(summary.mod.1$outmat)
v_errors
for(i in 1:27){
# Predict the y-values of the current fold with the
# "best" model with i predictors
MSE.mod.1 = predict.reg(mod.1, newdata = valid, id=i) #Fold==j, when j=1 then it takes the subset 1, etc.
#Basically saying i want the best model with i predictors! and im going to predict on the best validation data set
# Calculate the Mean Squared Error (MSE) estimate
# for the jth fold and best i-variable model, store it
# in the cv_errors matrix we created above
v_errors[i] = mean((valid$hgt-MSE.mod.1)^2)
}
v_errors
#Obtain the index number of which mean squared error (MSE) is the smallest.
min.1 = which.min(v_errors)[1]
min.1
#Plot the MSE
plot(v_errors, type='b', main="Graphed Mean Squared Error Values For Each Number of Predictors")
#Highlight the smallest one.
points(min.1, v_errors[min.1][1], col = "red", cex = 2, pch = 20)
#Indicate number of predictors as 12.
i=12
#Obtained predicted values with validation data set and linear model mod.1.
predict.hgt = predict.reg(object = mod.1, newdata = valid, id=i)
#Obtain MSE.
mse.hgt=mean((valid$hgt-predict.hgt)^2)
mse.hgt
mse.hgt
v_errors
predict.hgt
mse.hgt=mean((valid$hgt-predict.hgt)^2)
mse.hgt
#Indicate number of predictors as 12.
i=12
#Indicate id = i for function purposes.
id=i
#Combine training and validation data for refit step.
refit.dat=rbind(train,valid)
#Make a new model with training and validation data.
mod.1.refit = regsubsets(hgt ~ . + sex:bii.di + sex:che.de + sex:for.gi, data=refit.dat, nvmax=12)
#Summary of new model.
summary.mod.1.refit = summary(mod.1.refit)
#Obtain the coefficients from the champion model.
coefi.champ = as.data.frame(coef(mod.1.refit,id=id))
#Make into df.
coefi.champ.v = as.data.frame(t(coefi.champ))
#Change rownames.
rownames(coefi.champ.v) = c('coefficient.values')
coefi.champ.v
mse.hgt.test
mse.hgt
mse.hgt.test
b=bdims
set.seed(8929)
set.index = sample(c('train', 'validation', 'test'), prob = c(.5, .25, .25), replace = T, size = nrow(b))
train = b[set.index == 'train', ]
valid = b[set.index == 'validation', ]
test = b[set.index == 'test', ]
dim(train)
dim(valid)
dim(test)
b=bdims
dim(b)
#Create empty vector to store the values in later.
v_errors = rep(NA, 27)
#Make a model.
samp.mod = lm(hgt ~ . + sex*bii.di + sex*che.de + sex*for.gi, data=b)
summary(samp.mod)
#Use the training set to make a model.
mod.1 = regsubsets(hgt ~ . + sex:bii.di + sex:che.de + sex:for.gi, data=train, nvmax=27)
summary.mod.1 = summary(mod.1)
#Store summary of best models in outmat.reg.
outmat.reg = as.data.frame(summary.mod.1$outmat)
v_errors
for(i in 1:27){
# Predict the y-values of the current fold with the
# "best" model with i predictors
MSE.mod.1 = predict.reg(mod.1, newdata = valid, id=i) #Fold==j, when j=1 then it takes the subset 1, etc.
#Basically saying i want the best model with i predictors! and im going to predict on the best validation data set
# Calculate the Mean Squared Error (MSE) estimate
# for the jth fold and best i-variable model, store it
# in the cv_errors matrix we created above
v_errors[i] = mean((valid$hgt-MSE.mod.1)^2)
}
v_errors
knitr::opts_chunk$set(fig.align="center", fig.height=7, fig.width=7, collapse=TRUE, comment="", prompt=FALSE, echo = TRUE, cache=TRUE, autodep=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=50))
options(width=75)
#Use the training set to make a model.
mod.1 = regsubsets(hgt ~ . + sex:bii.di + sex:che.de + sex:for.gi, data=train, nvmax=27)
summary.mod.1 = summary(mod.1)
#Store summary of best models in outmat.reg.
outmat.reg = as.data.frame(summary.mod.1$outmat)
v_errors
for(i in 1:27){
# Predict the y-values of the current fold with the
# "best" model with i predictors
MSE.mod.1 = predict.reg(mod.1, newdata = valid, id=i) #Fold==j, when j=1 then it takes the subset 1, etc.
#Basically saying i want the best model with i predictors! and im going to predict on the best validation data set
# Calculate the Mean Squared Error (MSE) estimate
# for the jth fold and best i-variable model, store it
# in the cv_errors matrix we created above
v_errors[i] = mean((valid$hgt-MSE.mod.1)^2)
}
v_errors
knitr::opts_chunk$set(fig.align="center", fig.height=7, fig.width=7, collapse=TRUE, comment="", prompt=FALSE, echo = TRUE, cache=TRUE, autodep=TRUE, tidy=TRUE, tidy.opts=list(width.cutoff=50))
options(width=75)
#Read file.
gene.frame = read.csv(file = 'gene_exp.csv', row.names = 1)
#See Dimensions.
dim(gene.frame)
#Make df into matrix.
gene=as.matrix(gene.frame)
gene=as.matrix(gene.frame)
#Make a boxplot.
basic.gene = boxplot(gene, main="Distribution of gene expression",xlab='Gene',ylab='Gene Expression', cex.axis=0.5)
library(openintro)
library(leaps)
d = bdims[,c("wgt", "sex", "age", "sho.gi", "che.gi",
"thi.gi", "bic.gi", "for.gi", "kne.gi")]
test.frac = .25
set.seed(8909)
test.ind = sample(1:dim(d)[1], round(test.frac*dim(d)[1]))
test = d[test.ind, ]
dim(test)
train.val = d[-test.ind, ] #Training&Validation data set (without the test data)
dim(train.val)
dim(d)
k = 5        # number of folds
set.seed(2132)
folds = sample(1:k, nrow(train.val), replace = TRUE) #This splits the data into the 5 folds.
# Create a matrix to store the results of our upcoming calculations
cv_errors = matrix(NA, k, 8, dimnames = list(NULL, paste(1:8)))
# define a function to make prediction for an object
# that is the output of regsubsets()
#object is the output from regsubset
predict.reg = function(object,newdata,id){
form = as.formula(object$call[[2]]) # Extract the formula
# used when we called regsubsets()
mat = model.matrix(form,newdata)    # Build the design matrix
coefi = coef(object,id=id)          # Extract the coefficients of
# the id'th model
xvars = names(coefi)                # Pull out the names of the
# predictors used in the ith model
mat[,xvars]%*%coefi               # Make predictions using matrix
# multiplication; %*% does matrix multiplication
}
# Loop over all folds
for(j in 1:k){
best_fit = regsubsets(wgt~., data = train.val[folds!=j,], nvmax=dim(d)[2]-1)
# Inner loop iterates over each size i
for(i in 1:8){
# Predict the y-values of the current fold with the
# "best" model with i predictors
pred = predict.reg(best_fit, newdata = train.val[folds==j,], id=i)
# Calculate the Mean Squared Error (MSE) estimate
# for the jth fold and best i-variable model, store it
# in the cv_errors matrix we created above
cv_errors[j,i] = mean((train.val$wgt[folds==j]-pred)^2)
}
}
#So we have already decided the number of betas we need which is 4. Therefore we can use regsubset function...
#...To do the refit step.
#Since we only care about 4 predictors because we have already determine that our model is best with 4 betas...
#... nvmax is set to 4.
model.refit = regsubsets(wgt~., data = train.val, nvmax=4)
summary(model.refit)
model.refit
#Choose number of predictors wanted.
i=4
#Get coefficient values
coefi = coef(model.refit,id=i)
coefi
i=4 #Because I want a prediction of the y-values with 4 predictors (4 betas).
predic = predict.reg(object = model.refit, newdata = train.val, id=i)
#Find the mean squared error estimate by substracting the predicted values from the actual values
#... and squaring.
mse.error.estimate = mean((train.val[c('wgt')]-as.data.frame(predic))^2)
#Note to myself: i is the number of predictors (betas), j is the fold number.
#Choose # of predictors as 4.
i=4
#Obtain predicted data with the model refit and the test data.
predict.test.step = predict.reg(object = model.refit, newdata = test, id=i)
dim(predict.test.step)
#Find the mean squared error estimate by substracting the predicted values from the actual values
#... and squaring.
mse.test.step=mean((test[c('wgt')]-as.data.frame(predict.test.step))^2)
#I repeated the same code several times so I did not add an explanation for each line...
#...But created a model with the set 4 as the validation set and everything else the training set...
#... and then I changed the value of coefficients (i) for each section depending on what...
#... the quesiton was asking.
###For two predictors
j=4
#Set 4 is our validation set, everything else is our training set.
#Make model based on the training set.
fit.mod.2 = regsubsets(wgt~., data = train.val[folds!=j,], nvmax=dim(d)[2]-1)
#Make a fit mode that does not include set 4 with 8 predictors
summary(fit.mod.2)
#Coefficient values
i=2
coefi.2 = coef(fit.mod.2,id=i)
#MSE
predict.2 = predict.reg(object = fit.mod.2, newdata = train.val[folds==j,], id=i)
mse.2=mean((train.val$wgt[folds==j]-predict.2)^2)
###For three predictors
#Coefficient values
i=3
coefi.3 = coef(fit.mod.2,id=i)
#MSE
predict.3 = predict.reg(object = fit.mod.2, newdata = train.val[folds==j,], id=i)
mse.3=mean((train.val$wgt[folds==j]-predict.3)^2)
###For five predictors
#Coefficient values
i=5
coefi.5 = coef(fit.mod.2,id=i)
#MSE
predict.5 = predict.reg(object = fit.mod.2, newdata = train.val[folds==j,], id=i)
mse.5=mean((train.val$wgt[folds==j]-predict.5)^2)
#Note to myself: j goes from 1-5(folds) and i goes from 1-8 (betas).
library(openintro)
#Extract height data and store as a dataframe.
height.dat = bdims$hgt
length(height.dat)
#Make a histogram.
{hist(height.dat, freq=F, ylim=c(0,0.05), main='Distribution of the Height Data', xlab='Height (cm)',ylab='Density' )
hgt.mean = mean(bdims$hgt)
hgt.sd = sd(height.dat)
xseq = seq(from = min(bdims$hgt), to = max(height.dat), length = 100)
lines(y = dnorm(xseq, mean = hgt.mean, sd = hgt.sd), x = xseq, col = "blue")}
#Make a qqplot.
qqnorm(y = height.dat, main = "Normal Q-Q Plot for Height Data",
xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", cex.lab = 0.8,
cex.main = 1.0)
qqline(y = height.dat, col = "blue")
#Store bdims data in df b.
b = bdims
#Make sex variable a factor.
b$sex = as.factor(bdims$sex)
# make sure that you convert the variable `sex` into a factor.
library(GGally)
library(ggplot2)
ggmatrix = function(b){
# make sure that you convert the variable `sex` into a factor.
b$sex = as.factor(bdims$sex)
ggpairs(b, title="Relationship between height and other variables in bdims, factored by sex", mapping=ggplot2::aes(color = sex), upper = list(continuous = wrap("cor", size = 1)), ylab = "Body masurements in cm", binwidth = 0.5) + theme(axis.text.x = element_text(size = 5), axis.text.y = element_text(size = 5))}
b = bdims[,c(1:5,24, 25)]
ggmatrix(b)
b = bdims[,c(6:10,24, 25)]
ggmatrix(b)
b = bdims[,c(11:15,24, 25)]
ggmatrix(b)
b = bdims[,c(15:20,24, 25)]
ggmatrix(b)
b = bdims[,(21:25)]
ggmatrix(b)
b=bdims
set.seed(8929)
set.index = sample(c('train', 'validation', 'test'), prob = c(.5, .25, .25), replace = T, size = nrow(b))
train = b[set.index == 'train', ]
valid = b[set.index == 'validation', ]
test = b[set.index == 'test', ]
dim(train)
dim(valid)
dim(test)
b=bdims
dim(b)
#Create empty vector to store the values in later.
v_errors = rep(NA, 27)
#Make a model.
samp.mod = lm(hgt ~ . + sex*bii.di + sex*che.de + sex*for.gi, data=b)
summary(samp.mod)
#Use the training set to make a model.
mod.1 = regsubsets(hgt ~ . + sex:bii.di + sex:che.de + sex:for.gi, data=train, nvmax=27)
summary.mod.1 = summary(mod.1)
#Store summary of best models in outmat.reg.
outmat.reg = as.data.frame(summary.mod.1$outmat)
v_errors
for(i in 1:27){
# Predict the y-values of the current fold with the
# "best" model with i predictors
MSE.mod.1 = predict.reg(mod.1, newdata = valid, id=i) #Fold==j, when j=1 then it takes the subset 1, etc.
#Basically saying i want the best model with i predictors! and im going to predict on the best validation data set
# Calculate the Mean Squared Error (MSE) estimate
# for the jth fold and best i-variable model, store it
# in the cv_errors matrix we created above
v_errors[i] = mean((valid$hgt-MSE.mod.1)^2)
}
v_errors
#Obtain the index number of which mean squared error (MSE) is the smallest.
min.1 = which.min(v_errors)[1]
min.1
#Plot the MSE
plot(v_errors, type='b', main="Mean Squared Error Values For Each Number of Predictors")
#Highlight the smallest one.
points(min.1, v_errors[min.1][1], col = "red", cex = 2, pch = 20)
#Indicate number of predictors as 12.
i=12
#Obtained predicted values with validation data set and linear model mod.1.
predict.hgt = predict.reg(object = mod.1, newdata = valid, id=i)
#Obtain MSE.
mse.hgt=mean((valid$hgt-predict.hgt)^2)
mse.hgt
#Indicate number of predictors as 12.
i=12
#Indicate id = i for function purposes.
id=i
#Combine training and validation data for refit step.
refit.dat=rbind(train,valid)
#Make a new model with training and validation data.
mod.1.refit = regsubsets(hgt ~ . + sex:bii.di + sex:che.de + sex:for.gi, data=refit.dat, nvmax=12)
#Summary of new model.
summary.mod.1.refit = summary(mod.1.refit)
#Obtain the coefficients from the champion model.
coefi.champ = as.data.frame(coef(mod.1.refit,id=id))
#Make into df.
coefi.champ.v = as.data.frame(t(coefi.champ))
#Change rownames.
rownames(coefi.champ.v) = c('coefficient.values')
coefi.champ.v
#Indicate number of predictors as 12.
i=12
#Obtained predicted values with test data set and adjusted linear model.
pred.refit = predict.reg(mod.1.refit, newdata = test, id=i)
#Obtain MSE.
mse.hgt.test=mean((test$hgt-pred.refit)^2)
mse.hgt.test
mse.hgt
mod.1 = regsubsets(hgt ~ . + sex:bii.di + sex:che.de + sex:for.gi, data=train, nvmax=27)
summary.mod.1 = summary(mod.1)
outmat.reg = as.data.frame(summary.mod.1$outmat)
v_errors
v_errors
min.1 = which.min(v_errors)[1]
min.1
plot(v_errors, type='b', main="Mean Squared Error Values For Each Number of Predictors")
Plot the MSE
```{r}
min.1 = which.min(v_errors)[1]
min.1
plot(v_errors, type='b', main="Mean Squared Error Values For Each Number of Predictors")
points(min.1, v_errors[min.1][1], col = "red", cex = 2, pch = 20)
plot(v_errors, type='b', main="Mean Squared Error Values For Each Number of Predictors")
#Highlight the smallest one.
points(min.1, v_errors[min.1][1], col = "red", cex = 2, pch = 20)
min.1 = which.min(v_errors)[1]
v_errors
min.1
min.1
v_errors
hgt
b=bdims
b
hgt
b
summary(samp.mod)
samp.mod
bdims
dim(b)
bdims
bdims
samp.mod
b
bdims
b
b$sex = as.factor(bdims$sex)
b = bdims
b$sex = as.factor(bdims$sex)
b
b=bdims
b
set.seed(8929)
set.index = sample(c('train', 'validation', 'test'), prob = c(.5, .25, .25), replace = T, size = nrow(b))
train = b[set.index == 'train', ]
valid = b[set.index == 'validation', ]
test = b[set.index == 'test', ]
b=bdims
dim(b)
v_errors = rep(NA, 27)
samp.mod = lm(hgt ~ . + sex*bii.di + sex*che.de + sex*for.gi, data=b)
summary(samp.mod)
samp.mod
b
b$sex = as.factor(bdims$sex)
samp.mod = lm(hgt ~ . + sex*bii.di + sex*che.de + sex*for.gi, data=b)
summary(samp.mod)
samp.mod = lm(hgt ~ . + sex*bii.di + sex*che.de + sex*for.gi, data=b)
summary(samp.mod)
rm(list=ls()) # removes all prior objects
source('~/MyProjects/BioMin_HIS/RAnalysis/Scripts/CarbChem.R')
